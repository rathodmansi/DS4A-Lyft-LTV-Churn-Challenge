{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import data\n",
    "drivers = pd.read_csv(\"driver_ids.csv\",parse_dates = [\"driver_onboard_date\"])\n",
    "ride = pd.read_csv(\"ride_ids.csv\")\n",
    "time = pd.read_csv(\"ride_timestamps.csv\",parse_dates = [\"timestamp\"])\n",
    "# convert timestamp table to pivot table to aid with calculating difference later\n",
    "time_flat = time.pivot(index = \"ride_id\",columns = \"event\", values = \"timestamp\")\n",
    "# merge above time flat table with ride table\n",
    "time_ride = time_flat.merge(ride,how = \"inner\",on = \"ride_id\")\n",
    "time_ride = time_ride.sort_values([\"driver_id\",\"accepted_at\"])\n",
    "# merge with drivers table\n",
    "out = time_ride.merge(drivers,how = \"inner\",on = \"driver_id\")\n",
    "# out is final table, contains drivers information, ride information and timestamps information\n",
    "# calculate every ride value\n",
    "out['value'] =(1.75 + 2 + out['ride_distance']*0.000621371*1.15 + out['ride_duration']/60*0.22)*(1+out['ride_prime_time']/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loc[out['value']<=5,'value'] = 5\n",
    "out.loc[out['value']>=400,'value'] = 400\n",
    "out['value'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the week that each ride happens for each driver\n",
    "out['week'] = (out['picked_up_at'] - out['driver_onboard_date'].min()).dt.days//7\n",
    "# extract onboarding month information\n",
    "out['onboard_m'] = pd.DatetimeIndex(out['driver_onboard_date']).month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calculate recency for each driver**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the recency\n",
    "# below calculate the recency by days\n",
    "recency = out.groupby('driver_id')['arrived_at'].max().reset_index()\n",
    "recency.columns = ['driver_id','max_ride_time']\n",
    "recency['max_ride_time'].max()\n",
    "recency['recency'] = (recency['max_ride_time'].max()-recency['max_ride_time']).dt.days\n",
    "recency = recency[['driver_id','recency']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**retrieve march, apr, may cohorts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyjse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\tyjse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\tyjse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cohortMar = out[out['onboard_m'] == 3]\n",
    "cohortMar['week'] = (cohortMar['picked_up_at'] - cohortMar['driver_onboard_date'].min()).dt.days//7\n",
    "cohortApr = out[out['onboard_m'] == 4]\n",
    "cohortApr['week'] = (cohortApr['picked_up_at'] - cohortApr['driver_onboard_date'].min()).dt.days//7\n",
    "cohortMay = out[out['onboard_m'] == 5]\n",
    "cohortMay['week'] = (cohortMay['picked_up_at'] - cohortMay['driver_onboard_date'].min()).dt.days//7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tslearn.clustering import TimeSeriesKMeans\n",
    "# weekly_count = cohortMar.groupby(['driver_id','week'])\n",
    "# weekly_count = weekly_count['ride_id'].count().reset_index()\n",
    "# weekly_count = weekly_count.pivot(index = \"driver_id\",columns = \"week\", values = \"ride_id\")\n",
    "# weekly_count.fillna(0, inplace = True)\n",
    "# model = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\")\n",
    "# model.fit(weekly_count)\n",
    "# pred = model.predict(weekly_count)\n",
    "# Count = pd.DataFrame(weekly_count)\n",
    "# Count['count_cluster'] = pred\n",
    "# Count = Count.reset_index()\n",
    "# Count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Calculate weekly driving count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_count_mar = cohortMar.groupby(['driver_id','week'])\n",
    "weekly_count_mar = weekly_count_mar['ride_id'].count().reset_index()\n",
    "weekly_count_mar = weekly_count_mar.pivot(index = \"driver_id\",columns = \"week\", values = \"ride_id\")\n",
    "weekly_count_mar.fillna(0, inplace = True)\n",
    "# kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "# kmeans.fit(weekly_count)\n",
    "# pred = kmeans.predict(weekly_count)\n",
    "# Count = pd.DataFrame(weekly_count)\n",
    "# Count['count_cluster'] = pred\n",
    "# Count = Count.reset_index()\n",
    "# Count['count_mean'] = Count.iloc[:,2:14].mean(axis=1)\n",
    "# Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calculate weekly driving value generated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_value_mar = cohortMar.groupby(['driver_id','week'])\n",
    "weekly_value_mar = weekly_value_mar['value'].sum().reset_index()\n",
    "weekly_value_mar = weekly_value_mar.pivot(index = \"driver_id\",columns = \"week\", values = \"value\")\n",
    "weekly_value_mar.fillna(0, inplace = True)\n",
    "# kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "# kmeans.fit(weekly_value)\n",
    "# pred = kmeans.predict(weekly_value)\n",
    "# Value = pd.DataFrame(weekly_value)\n",
    "# Value['value_cluster'] = pred\n",
    "# Value = Value.reset_index()\n",
    "# Value['value_mean'] = Value.iloc[:,2:14].mean(axis = 1)\n",
    "# Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. merge value and count table, recency table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_v_mar = recency.merge(weekly_count_mar, how = \"inner\", on = 'driver_id')\n",
    "c_v_mar = c_v_mar.merge(weekly_value_mar, how = 'inner', on = 'driver_id')\n",
    "c_v_mar.set_index('driver_id',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. run kmeans on the c_v dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "kmeans.fit(c_v)\n",
    "pred = kmeans.predict(c_v)\n",
    "Cluster = pd.DataFrame(c_v)\n",
    "Cluster['cluster'] = pred\n",
    "Cluster = Cluster.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. run kmeans on centalized c_v data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_v_mar = recency.merge(weekly_count_mar, how = \"inner\", on = 'driver_id')\n",
    "c_v_mar = c_v_mar.merge(weekly_value_mar, how = 'inner', on = 'driver_id')\n",
    "c_v_mar.set_index('driver_id',inplace = True)\n",
    "origin_mar = recency.merge(weekly_count_mar, how = \"inner\", on = 'driver_id')\n",
    "origin_mar = origin_mar. merge(weekly_value_mar, how = 'inner', on = 'driver_id')\n",
    "origin_mar.set_index('driver_id',inplace = True)\n",
    "for i in c_v_mar.columns:\n",
    "    c_v_mar[i] = (c_v_mar[i] - c_v_mar[i].mean())/c_v_mar[i].std()\n",
    "import random\n",
    "random.seed(2020)\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "kmeans.fit(c_v_mar)\n",
    "pred = kmeans.predict(c_v_mar)\n",
    "Cluster_mar = pd.DataFrame(c_v_mar)\n",
    "Cluster_mar['cluster'] = pred\n",
    "Cluster_mar = Cluster_mar.reset_index()\n",
    "Cluster_mar = Cluster_mar[['driver_id','cluster']]\n",
    "Cluster_mar = Cluster_mar.merge(origin_mar, how = \"inner\", on = 'driver_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. calculate some matrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_mar['valueq10'] = Cluster_mar.iloc[:,16:29].quantile(q=0.1,axis = 1)\n",
    "Cluster_mar['valueq90'] = Cluster_mar.iloc[:,16:29].quantile(q=0.9,axis = 1)\n",
    "Cluster_mar['valueq50'] = Cluster_mar.iloc[:,16:29].quantile(q=0.5,axis = 1)\n",
    "Cluster_mar['value_mean'] = Cluster_mar.iloc[:,16:29].sum(axis = 1)/13\n",
    "sns.scatterplot(data = Cluster_mar, x ='valueq10', y = 'value_mean', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = Cluster_mar, x ='valueq10', y = 'valueq90', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster_mar[Cluster_mar['cluster'] ==2].iloc[:,0:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the normalized results, the group classified as 0, is the possible churn group;\n",
    "the group classified as 1, there is potential to churn, and weekly drive count is not consistent\n",
    "the group classified as 2, there is small possibility to churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Running through cohort Apr and May**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_count_apr = cohortApr.groupby(['driver_id','week'])\n",
    "weekly_count_apr = weekly_count_apr['ride_id'].count().reset_index()\n",
    "weekly_count_apr = weekly_count_apr.pivot(index = \"driver_id\",columns = \"week\", values = \"ride_id\")\n",
    "weekly_count_apr.fillna(0, inplace = True)\n",
    "weekly_value_apr = cohortApr.groupby(['driver_id','week'])\n",
    "weekly_value_apr = weekly_value_apr['value'].sum().reset_index()\n",
    "weekly_value_apr = weekly_value_apr.pivot(index = \"driver_id\",columns = \"week\", values = \"value\")\n",
    "weekly_value_apr.fillna(0, inplace = True)\n",
    "c_v_apr = recency.merge(weekly_count_apr, how = \"inner\", on = 'driver_id')\n",
    "c_v_apr = c_v_apr.merge(weekly_value_apr, how = 'inner', on = 'driver_id')\n",
    "c_v_apr.set_index('driver_id',inplace = True)\n",
    "origin_apr = recency.merge(weekly_count_apr, how = \"inner\", on = 'driver_id')\n",
    "origin_apr = origin_apr. merge(weekly_value_apr, how = 'inner', on = 'driver_id')\n",
    "origin_apr.set_index('driver_id',inplace = True)\n",
    "for i in c_v_apr.columns:\n",
    "    c_v_apr[i] = (c_v_apr[i] - c_v_apr[i].mean())/c_v_apr[i].std()\n",
    "random.seed(2021)\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "kmeans.fit(c_v_apr)\n",
    "pred = kmeans.predict(c_v_apr)\n",
    "Cluster_apr = pd.DataFrame(c_v_apr)\n",
    "Cluster_apr['cluster'] = pred\n",
    "Cluster_apr = Cluster_apr.reset_index()\n",
    "Cluster_apr = Cluster_apr[['driver_id','cluster']]\n",
    "Cluster_apr = Cluster_apr.merge(origin_apr, how = \"inner\", on = 'driver_id')\n",
    "Cluster_apr['valueq10'] = Cluster_apr.iloc[:,16:29].quantile(q=0.1,axis = 1)\n",
    "Cluster_apr['valueq90'] = Cluster_apr.iloc[:,16:29].quantile(q=0.9,axis = 1)\n",
    "Cluster_apr['valueq50'] = Cluster_apr.iloc[:,16:29].quantile(q=0.5,axis = 1)\n",
    "Cluster_apr['value_mean'] = Cluster_apr.iloc[:,16:29].sum(axis = 1)/12\n",
    "#sns.scatterplot(data = Cluster_apr, x ='valueq10', y = 'valueq90', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. May cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_count_may = cohortApr.groupby(['driver_id','week'])\n",
    "weekly_count_may = weekly_count_may['ride_id'].count().reset_index()\n",
    "weekly_count_may = weekly_count_may.pivot(index = \"driver_id\",columns = \"week\", values = \"ride_id\")\n",
    "weekly_count_may.fillna(0, inplace = True)\n",
    "weekly_value_may = cohortApr.groupby(['driver_id','week'])\n",
    "weekly_value_may = weekly_value_may['value'].sum().reset_index()\n",
    "weekly_value_may = weekly_value_may.pivot(index = \"driver_id\",columns = \"week\", values = \"value\")\n",
    "weekly_value_may.fillna(0, inplace = True)\n",
    "c_v_may = recency.merge(weekly_count_may, how = \"inner\", on = 'driver_id')\n",
    "c_v_may = c_v_may.merge(weekly_value_may, how = 'inner', on = 'driver_id')\n",
    "c_v_may.set_index('driver_id',inplace = True)\n",
    "origin_may = recency.merge(weekly_count_may, how = \"inner\", on = 'driver_id')\n",
    "origin_may = origin_may. merge(weekly_value_may, how = 'inner', on = 'driver_id')\n",
    "origin_may.set_index('driver_id',inplace = True)\n",
    "for i in c_v_may.columns:\n",
    "    c_v_may[i] = (c_v_may[i] - c_v_may[i].mean())/c_v_may[i].std()\n",
    "random.seed(2020)\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "kmeans.fit(c_v_may)\n",
    "pred = kmeans.predict(c_v_may)\n",
    "Cluster_may = pd.DataFrame(c_v_may)\n",
    "Cluster_may['cluster'] = pred\n",
    "Cluster_may = Cluster_may.reset_index()\n",
    "Cluster_may = Cluster_may[['driver_id','cluster']]\n",
    "Cluster_may = Cluster_may.merge(origin_may, how = \"inner\", on = 'driver_id')\n",
    "Cluster_may['valueq10'] = Cluster_may.iloc[:,16:29].quantile(q=0.1,axis = 1)\n",
    "Cluster_may['valueq90'] = Cluster_may.iloc[:,16:29].quantile(q=0.9,axis = 1)\n",
    "Cluster_may['valueq50'] = Cluster_may.iloc[:,16:29].quantile(q=0.5,axis = 1)\n",
    "Cluster_may['value_mean'] = Cluster_may.iloc[:,16:29].sum(axis = 1)/12\n",
    "#sns.scatterplot(data = Cluster_may, x ='valueq10', y = 'valueq90', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster_apr[Cluster_apr['cluster'] == 1].iloc[:,0:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. put three cohorts together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Cluster_mar[['driver_id','cluster','valueq10','valueq90','value_mean','recency']]\n",
    "b = Cluster_apr[['driver_id','cluster','valueq10','valueq90','value_mean','recency']]\n",
    "c = Cluster_may[['driver_id','cluster','valueq10','valueq90','value_mean','recency']]\n",
    "a= a.append(b)\n",
    "churn = a.append(c)\n",
    "#sns.scatterplot(data = churn, x ='valueq10', y = 'valueq90', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(data = churn, x ='recency', y = 'valueq90', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Compute some other factors relevant to churn behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. First week rides and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['first_week'] = out['driver_onboard_date'] + timedelta(days=7)\n",
    "out_first_week = out[out['accepted_at']<= out['first_week']]\n",
    "out_first_week_count = out_first_week.groupby('driver_id').ride_id.count().reset_index()\n",
    "out_first_week_count.rename(columns = {'ride_id':'first_week_frequency'}, inplace = True)\n",
    "first_week_value = out_first_week.groupby('driver_id').value.sum().reset_index()\n",
    "first_week_value.rename(columns = {'value':'first_week_value'}, inplace = True)\n",
    "#merge with churn data set\n",
    "churn = churn.merge(out_first_week_count, how = \"inner\", on = \"driver_id\")\n",
    "churn = churn.merge(first_week_value, how = \"inner\", on = \"driver_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(data = churn, x ='first_week_frequency', y = 'first_week_value', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.boxplot(y='first_week_frequency', x='cluster', data=churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.boxplot(y='first_week_value', x='cluster', data=churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. calculate driver response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyjse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cohortMar['response'] = (cohortMar['accepted_at'] - cohortMar['requested_at']).dt.seconds\n",
    "# calculate means response for each driver\n",
    "response_time = cohortMar.groupby('driver_id').response.mean().reset_index()\n",
    "Cluster2 = Cluster2.merge(response_time, how = 'left', on = 'driver_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
